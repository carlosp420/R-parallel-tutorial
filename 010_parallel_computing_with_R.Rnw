\section{Parallel computing with \R}
%-----------------------------------



\subsection{When can parallel computing be used?}
%------------------------------------------------



Parallel computing enables to do calculations simultaneously on several cores.
If a desktop computer has a multi-core processor, it usually uses only one
core at a time to run an \R\ session. However, for certain analyses, it is 
possible to take advantage of the presence of several cores and to run 
independent parts of an analysis on different cores, simultaneously
(figure \ref{fig.comparison.single.multiple.cores}).

Not all analyses can be performed using parallel computing. Some analyses 
require sequential steps, and each step depends on the result of the previous
one. In this case, it is not possible to distribute the calculation load on 
independent processes. However, many analyses with heavy calculation requirements 
consist of a few analysis steps which are repeated a large number of
time, independently, on different inputs. Here are some examples:

\begin{itemize}
  \item G$_{st}$ values are available along the genome for your favourite 
  species. You want to perform a kernel smoothing and some permutation 
  testing to detect regions with significantly high Gst on each chromosome. 
  The analyses for each chromosome are independent and can be performed on 
  separate cores.
  \item You want to test the robustness of your results by bootstrapping.
  You have to rerun your analysis a large number of time on resampled datasets
  produced by shuffling the original dataset. Each analysis on a resampled
  dataset is independent from the others, and the bootstrap simulations can
  be distributed among several cores. 
\end{itemize}

After the parallel step, another step is often necessary to gather the results
produced independently (e.g. concatenate the results for each chromosome into
a single table or pool the results of all the bootstrap simulations to 
calculate p-values). However, the most time-consuming step is usually the one
which can be distributed on multiple cores.

\begin{figure}
  \begin{center}
    \includegraphics[width=\linewidth]{../figures/analysis_all.pdf}
  \end{center}
  \caption[Comparison of performances between single and multiple core runs]
  {Comparison of performances between single and multiple core runs. One 
  orange circle represents one cpu.time unit (e.g. the amount of 
  calculations performed by one core during one hour). In A, a simple 
  analysis needs one cpu.time unit to be run (e.g. a one hour run with one 
  core). In B, a longer analysis is performed, which needs 5 cpu.time units 
  (e.g. a five hour run with one core). In this case, each step of the 
  analysis depends on the previous one, and the run has to be sequential. In 
  C, however, the analysis is the same as in A but is performed on a larger 
  amount of input data. The analysis requires three cpu.time units (the 
  three orange circles, e.g. a three hour run with one core). Since the 
  analysis performed in C is actually independent for each data packet, it 
  can be parallelized as shown in D and run in only one time unit if three 
  cores are used (e.g. a one hour parallel run with three cores). The amount 
  of calculations performed is the same as in C (three cpu.time units), but 
  the elapsed time is shorter since three cores were working at the same 
  time. The sequential analysis in B cannot be made faster by 
  parallelization since each analysis step depends on the previous step 
  (true sequential analysis), but if the analysis pipeline has to be run on 
  several data packets independently then it can be parallelized to make the 
  analysis run faster when there is more data as shown in E (e.g a five hour 
  run with three cores instead of a 15 hour run with one core).}
  \label{fig.comparison.single.multiple.cores}
\end{figure}

One quick-and-dirty way for using several cores with \R\ is to manually start as 
many \R\ sessions as available cores. However, when scaling an analysis to 16, 32 
or more cores on a computer grid, manually starting and managing \R\ scripts 
is not a good option. A much more simple, robust and elegant solution is to 
use one of the \R\ packages for parallel computing.



\subsection{Approach to parallel computing with \R\ used in this document}
%-----------------------------------------------------------------------



The aim of this document is to provide a step-by-step guide of how to use 
one approach which was tested and worked for some analyses performed on a 
local computer with multiple cores and on the CSC servers (\url
{http://research.csc.fi/}). A lot of detailed resources are already 
available concerning parallel computing with \R. One can look through the 
documentation of the corresponding packages or have a look on GitHub and 
search for \href {https://github.com/search?q=r+parallel&ref=cmdform}{\texttt
{'parallel R'}}.



\subsubsection{Packages and resources for parallel computing with \R}
%--------------------------------------------------------------------



A list of useful packages can be found on the \href
{http://cran.r-project.org/web/views/HighPerformanceComputing.html}{CRAN 
webpage} concerning \R\ and high performance computing. There are  many 
different options; I will focus on the \texttt{snow} package since I managed 
to use this one both on a CSC server (Taito) and on my desktop computer and 
its setup is quite simple.



\subsubsection{Using \emph{apply} functions in \R code}
%------------------------------------------------------



The approach explained in this document relies on the use of the \emph 
{apply} family of functions in \R. Those functions are \texttt{apply}, 
\texttt{sapply} and \texttt{lapply} and are part of the \texttt{base} 
package. They are seldom used by \R\ users when they just start to learn \R, 
but they are of considerable interest when one becomes more familiar with 
the language. The documentation can be accessed by typing \texttt{?apply} or 
\texttt{?lapply} in \R.

Each function of this family takes as an input an object made of several 
elements (e.g. a vector) and applies a function on each element of this 
object. Here is a very simple example using a vector of numbers, and 
returning the square of each number:

<<label=sapply.simple.example>>=
# vector object with four elements
v = c(1, 2, 3, 4)
# show v
v
# function returning the square of its input
f = function(x) { x^2 }
# show f
f
# example of f usage
f(5)
f(6)
# sapply can apply the f function to each element of v
sapply(v, f)
# now we want to apply f to the integers from 5 to 15
# a range of integers can be specified using the ":" notation
5:15
sapply(5:15, f)
@

\texttt{apply}, \texttt{sapply} and \texttt{lapply} differ in the type of 
objects on which they work: \texttt{sapply} will apply a function on each 
element of a \emph{vector}, \texttt{lapply} on each element of a \emph{list} 
and \texttt{apply} on each row or each column of a \emph{matrix}. Since 
those functions work on each element of an object independently from the 
other elements, they are easily amenable to parallelization. This is done in 
a very simple manner with the \texttt{snow} package: the function names just 
have to be replaced with the corresponding \texttt{parApply}, 
\texttt{parSapply} or \texttt{parLapply} functions. 

An efficient use of \texttt{apply}, \texttt{sapply} and \texttt{lapply} 
requires a good understanding of the \texttt{vector}, \texttt{list} and 
\texttt{matrix} or \texttt{data.frame} objects. In particular, we will be 
using the list object which is very versatile but might seem a bit daunting 
to users without any prior experience with it. Using the \texttt{apply} 
family of functions tends to produce cleaner and better code - cleaner since 
the user has to structure the data efficiently into e.g. lists and to divide 
its code into separate functions, and better because the code is easier to 
read and to debug - and is of interest even for code which is not intended 
to be used on multiple cores.

Running code which already uses any \texttt{apply} family function on 
multiple cores requires only a few modifications, such as setting up the 
core cluster at the beginning of the script and converting the \emph{apply} 
calls to the corresponding \emph{parApply} calls. Running code which is 
already working fine but without using any \emph{apply} function yet 
requires a bit more work to refactor it before being able to use it on 
multiple cores, but even then the amount of work is relatively minor and 
involves mainly a wrapping of the analysis into a main function and a 
storage of the data into a suitable list structure.



\subsubsection{What will be done in this tutorial}
%-------------------------------------------------



This tutorial will focus on using the \texttt{list} object to store data and
the \texttt{lapply} and \texttt{parLapply} functions to apply a function on 
each element of a list. The first part of the tutorial is about the 
\texttt{list} object structure and how to use it. The next part will describe
how to run a script using \texttt{lapply} on several cores on a local computer.
Finally, the last part will show how to run parallel code on CSC servers.

The appendix contains some useful material concerning code benchmarking.



