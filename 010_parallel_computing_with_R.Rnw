\section{Parallel computing with R}
%----------------------------------



\subsection{When can parallel computing be used?}
%------------------------------------------------



Parallel computing enables to do calculations simultaneously on several cores.
If a desktop computer has a multi-core processor, it usually uses only one
core at a time to run an R session. However, for certain analyses, it is 
possible to take advantage of the presence of several cores and to run 
independent parts of an analysis on different cores, simultaneously.
One quick-and-dirty way of using several cores is to manually start as many
R sessions as available cores; a much more simple, robust and elegant solution
is to use one of the R packages for parallel computing. When scaling an 
analysis on 16, 32 or more cores on a computer grid, manually starting and 
managing R scripts is not a good option.

Not all analyses can be performed using parallel computing. Some analyses 
reauire sequential steps, and each step depends on the result of the previous
one. In this case, it is not possible to distribute the calculation load on 
independent processes. However, analyses with heavy calculation requirements 
often consist of a few analysis steps which are repeated a large number of
time, independently, on different input. Here are some examples:

\begin{itemize}
  \item G$_{st}$ values are available along the genome for your favourite species.
  You want to perform a kernel smoothing and some permutation testing to
  detect significantly high Gst for each chromosome. The analyses for each
  chromosome are independent and can be performed on separate cores.
  \item You want to test the robustness of your results by bootstrapping.
  You have to rerun your analysis a large number of time on resampled datasets
  produced from the original observed dataset. Each analysis on a resampled
  dataset is independent of the others, and all the bootstrap simulations can
  be distributed among several cores. 
\end{itemize}

After the parallel step, another step is often necessary to gather the results
produced independently (e.g. concatenate the results for each chromosome into
a single table or pool the results of all the bootstrap simulations to 
calculate p-values). However, the most time-consuming step is usually the one
which can be distributed on multiple cores.



\subsection{Approach to parallel computing with R used in this document}
%-----------------------------------------------------------------------



\subsubsection{Packages}
%-----------------------



A list of useful packages can be found on the CRAN webpage concerning R and 
high performance computing 
(\url{http://cran.r-project.org/web/views/HighPerformanceComputing.html}).
There are  many different options; I will focus on the \texttt{snow} package
since I managed to use this one both on a CSC server (Taito) and on my desktop
computer and its setup is quite simple.



\subsubsection{Using \texttt{lapply} in R code}
%----------------------------------------------



The approach explained in this document relies on the use on the \texttt{lapply}
R function. This function is part of the \texttt{base} package but is seldom 
used by R users at the beginning of their learning.

\texttt{lapply} applies a function to each element of a list or of a 
vector (see \texttt{?lapply} for the complete documentation). An efficient use 
of \texttt{lapply} requires a good understanding of the \texttt{list} objects,
which is very easy to acquire but might seem daunting to users without 
any prior experience with this structure. Using \texttt{lapply} tends to 
produce cleaner and better code - cleaner since the user has to structure the
data into lists and to divide its code into separate functions, and better 
because the code is easier to read and to debug - and is of interest even for
code which is not intended to be used on multiple cores.

Running code which already uses \texttt{lapply} on multiple cores requires only
a few modifications. Running code which is already working fine but without
using \texttt{lapply} requires a bit more work to refactor it before being
able to use it on multiple cores, but even then the amount of work is 
relatively minor and involves mainly a wrapping of the analysis into a main 
function and a storage of the data into a suitable list structure.
