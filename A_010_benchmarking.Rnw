\chapter{Benchmarking code}
%--------------------------


\label{section_rbenchmark}



Benchmarking is useful to determine how fast some pieces of code are run, and
to compare different implementations of the same analysis to choose the 
fastest one. The \texttt{rbenchmark} package provides a function to easily 
benchmark R code.

Benchmarked code is usually a small piece of code which is fast to run
in itself but that is used a large number of times in an analysis.
If this piece of code can be improved to run faster, the whole analysis run
time can be substantially improved. Since one unit run of the piece of code 
of interest is usually very fast, it is hard to compare two different 
implementations of it just with a single unit run. The \texttt{benchmark} 
function enables to replicate unit runs of different implementations of the 
code of interest and provides a convenient summary
comparison of the execution times. For very small pieces of code, the 
\texttt{microbenchmark} package is also interesting.

Here is a small example of the use of the \texttt{benchmark} function in
which we compare two different implementation of a function to 
calculate the mean of a vector.

<<>>=
# R built-in function
mean.R.function = function(x) {
  mean(x)
}
# manual coding
mean.manual.loop = function(x) {
  x.sum = 0
  for (i in x) {
    x.sum = x.sum + i
  }
  x.sum / length(x)
}
# prepare a test vector
x = rnorm(1000)
# run the benchmark
library(rbenchmark)
bmk = benchmark(mean.R.function(x), mean.manual.loop(x),
                columns = c("test", "replications", 
                            "elapsed", "relative"),
                order = "relative",
                replications = 1000)
bmk
@

The functions are run \Sexpr{bmk$replications[1]} times. The function using
the R built-in function is the fastest one, and the function using the 
manually-coded loop is \Sexpr{round(bmk$relative[2], digits = 0)} times
slower than the R built-in.

We can use this \texttt{benchmark} function to compare the two previous
methods to calculate the mean petal length of each iris species.

<<>>=
# first approach
calculate.mean.petal.length = function(x) {
  mean(x$Petal.Length)  
}
simple.approach = function(dataset) {
  # prepare the data frame to store the results
  mean.lengths = data.frame(sp = vector(), petal.length = vector())
  row.i = 1
  # loop through the species
  for (sp in levels(dataset$Species)) {
    # get the data subset for a given species
    sub.data = subset(dataset, dataset$Species == sp)
    # apply the function
    result = calculate.mean.petal.length(sub.data)
    # store the result
    mean.lengths[row.i, ] = c(sp, result)
    row.i = row.i + 1
  }
  # return the results
  mean.lengths
}
simple.approach(iris)
# second approach
lapply.approach = function(dataset) {
  # prepare the data
  sp.data = list()
  for (sp in levels(dataset$Species)) {
    sp.data[[sp]] = subset(dataset, dataset$Species == sp)
  }
  # apply the function to each element of the list
  mean.lengths = lapply(sp.data, calculate.mean.petal.length)
  # return the results
  mean.lengths
}
lapply.approach(iris)
# benchmark
bmk = benchmark(simple.approach(iris), lapply.approach(iris),
                columns = c("test", "replications", 
                            "elapsed", "relative"),
                order = "relative",
                replications = 100)
bmk
@

The approach using \texttt{lapply} is a bit faster than the other one.
However, we should bear in mind that this is a very simple example and 
that the main gain in time resulting from \texttt{lapply} is the possibility
to run the code on multiple cores at the same time.



\chapter{Benchmarking - speed gain with cluster over serial function}
%--------------------------------------------------------------------


% (from 030_parallel_computing_on_a_local_computer)


We can compare the speed improvement with the \texttt{rbenchmark library}
(section \ref{section_rbenchmark}). We first have to define the two functions
that will be benchmarked. The task for which they will be compared is 
the bootstrap analysis above (i.e., generate \Sexpr{n.bootstrap} bootstrapped
slope values).

<<>>=
# serial function
run.serial.bootstrap = function() {
  
  # generate the bootstrap resamples
  resamples = list()
  for (i in 1:n.bootstrap) {
    # first draw randomly the row indices for the resample
    resampled.rows = sample(1:nrow(cars), replace = T)
    # then store the resample data into the list
    resamples[[i]] = cars[resampled.rows, ]
  }
  
  # function to get the linear model
  linear.fit = function(car.data) {
    # calculate speed.squared
    car.data$speed.squared = car.data$speed ^ 2
    # linear model
    model.resample = lm(car.data$dist ~ car.data$speed.squared)
    # return the model
    model.resample
  }
  
  # run the linear model to get the bootstrapped slopes
  slopes = vector()
  for (i in 1:n.bootstrap) {
    # run the model on one resample
    model = linear.fit(resamples[[i]])
    # store the bootstrapped slope
    slopes[i] = model$coefficients[2]
  }
  
  # return the bootstrapped slopes
  slopes
  
}
@

For the parallel function, we adopt a slightly different approach compared to
before: since the serial function generates the bootstrap resamples on the fly,
it is also fair that the bootstrap function generates it the same way, and does
not make use of a pre-calculated bootstrap resample dataset.

We also make use of a function \texttt{linear.fit} inside the function itself,
just to keep the same overhead of function calls between the serial and the
parallel approach.

<<>>=
# parallel function
parallel.bootstrap = function(i) {
  # We specify one argument i because parLapply will send an 
  # argument to the function for each simulation.
  
  ## generate a bootstrap resample
  # first draw randomly the row indices for the resample
  resampled.rows = sample(1:nrow(cars), replace = T)
  # then store the resample data into the list
  resample = cars[resampled.rows, ]
  
  # function to get the linear model
  linear.fit = function(car.data) {
    # calculate speed.squared
    car.data$speed.squared = car.data$speed ^ 2
    # linear model
    model.resample = lm(car.data$dist ~ car.data$speed.squared)
    # return the model
    model.resample
  }
  
  # run the linear model
  model = linear.fit(resample)
  
  # return the slope
  model$coefficients[2]
  
}
@

This function has to be used with a cluster.

<<>>=
run.parallel.bootstrap = function() {
  # initialize cluster
  cluster = makeSOCKcluster(rep("localhost", n.cores))
  # prepare the simulation indices
  sim.i = as.list(1:n.bootstrap)
  # run the simulation
  bootstrapped.slopes = parLapply(cluster, sim.i, parallel.bootstrap)
  # stop the cluster
  stopCluster(cluster)
  # return the bootstrapped slopes as a vector, not a list
  unlist(bootstrapped.slopes, use.names = F)
}
@

Now let's run the benchmark.

<<>>=
library(rbenchmark)
bmk = benchmark(run.serial.bootstrap(), run.parallel.bootstrap(),
                columns = c("test", "replications", 
                            "elapsed", "relative"),
                order = "relative",
                replications = 2)
bmk
@

In this case (with \Sexpr{n.cores} cores), the parallel run is actually longer 
than the serial run ! This is probably due to the fact that the analysis in 
itself is extremely fast, and the overhead of setting up the cluster and 
dispatching the calculation to each node is actually more costly than the 
analysis itself. However, when the analysis is a long one, the overhead from 
using the cluster will become negligible compared to the analysis time itself, 
and the time for a run will be divided by the number of cores.
